{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Image retrieval with prompts with MetaClip\n"
=======
    "##  Image retrieval with prompts with MetaClip"
>>>>>>> c6fa8c57de22142415714cfd27998149e99f6e50
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.image_transforms import rescale\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "#Only needed in Google colab\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most of the libraries in this code are not supported by higher versions of Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/metaclip-b16-fullcc2.5b\")\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(\"facebook/metaclip-b16-fullcc2.5b\",  torch_dtype=torch.float16).to(device)\n",
    "model = torch.compile(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/metaclip-b16-fullcc2.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"zh-plus/tiny-imagenet\")\n",
    "\n",
    "#Display an image\n",
    "display(dataset['valid'][0]['image'])\n",
    "\n",
    "#Keep only validation set\n",
    "valid_dataset = dataset[\"valid\"]\n",
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a vector to FAISS index\n",
    "def add_vector_to_index(embedding, index):\n",
    "    #convert embedding to numpy\n",
    "    vector = embedding.detach().cpu().numpy()\n",
    "    #Convert to float32 numpy\n",
    "    vector = np.float32(vector)\n",
    "    #Normalize vector\n",
    "    faiss.normalize_L2(vector)\n",
    "    #Add to index\n",
    "    index.add(vector)\n",
    "\n",
    "#Extract features of a given image\n",
    "def extract_features_clip(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        return image_features\n",
    "\n",
    "#FAISS index\n",
    "index = faiss.IndexFlatL2(512)\n",
    "\n",
    "#Process the dataset to extract all features and store in index\n",
    "for image in tqdm(valid_dataset):\n",
    "    clip_features = extract_features_clip(image['image'])\n",
    "    add_vector_to_index(clip_features,index)\n",
    "\n",
    "#Write index locally. Not needed after but can be useful for future retrieval\n",
    "faiss.write_index(index,\"metaclip.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdn.pariscityvision.com/library/image/5144.jpg\"\n",
    "input_image = Image.open(requests.get(url, stream=True).raw)\n",
    "display(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features of the input image\n",
    "input_features = extract_features_clip(input_image)\n",
    "\n",
    "#Preprocess the vector before searching the FAISS index\n",
    "input_features_np = input_features.detach().cpu().numpy()\n",
    "input_features_np = np.float32(input_features_np)\n",
    "faiss.normalize_L2(input_features_np)\n",
    "\n",
    "#Search the top 5 images\n",
    "distances, indices = index.search(input_features_np, 1)\n",
    "print('distances',distances)\n",
    "print('indices' ,indices)\n",
    "\n",
    "#For each top-5 results, compute similarity score between 0 and 1, print indice, similarity score and display image\n",
    "for i,v in enumerate(indices[0]):\n",
    "    sim = (1/(1+distances[0][i])*100)\n",
    "    print(f\"Similarity score: {sim}\")\n",
    "    img_resized = valid_dataset[int(v)]['image'].resize((200, 200))\n",
    "    display(img_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MetaCLIP with an Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_metaclip import MetaCLIP\n",
    "from autodistill.detection import CaptionOntology\n",
    " \n",
    "classes = [\"empire state building\", \"chrysler building\", \"one world trade center building\"]\n",
    " \n",
    "base_model = MetaCLIP(\n",
    "  ontology=CaptionOntology({\n",
    "    \"The Empire State Building\": \"empire state building\",\n",
    "    \"The Chrysler Building\": \"chrysler building\",\n",
    "    \"The One World Trade Center Building\": \"one world trade center building\"\n",
    "  })\n",
    ")\n",
    " \n",
    "results = base_model.predict(file_path, confidence=0.5)\n",
    "print(results)\n",
    " \n",
    "top_result = classes[results.class_id[0]]\n",
    "print(top_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Compare Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_metaclip import MetaCLIP\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Create an instance of MetaCLIP\n",
    "base_model = MetaCLIP(None)\n",
    "\n",
    "# Define the text and image to compare\n",
    "text = \"the chrysler building\"\n",
    "url = \"https://cdn.pariscityvision.com/library/image/5144.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Embed the text and image\n",
    "text_embedding = base_model.embed_text(text)\n",
    "image_embedding = base_model.embed_image(image)\n",
    "\n",
    "# Compare the text and image embeddings\n",
    "similarity_score = base_model.compare(text_embedding, image_embedding)\n",
    "\n",
    "print(f\"Similarity score: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_metaclip import MetaCLIP\n",
    "\n",
    "# define an ontology to map class names to our MetaCLIP prompt\n",
    "# the ontology dictionary has the format {caption: class}\n",
    "# where caption is the prompt sent to the base model, and class is the label that will\n",
    "# be saved for that caption in the generated annotations\n",
    "# then, load the model\n",
    "base_model = MetaCLIP(\n",
    "    ontology=CaptionOntology(\n",
    "        {\n",
    "            \"person\": \"person\",\n",
    "            \"a forklift\": \"forklift\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "base_model = MetaCLIP(None)\n",
    "results = base_model.predict(Image.open(requests.get(url, stream=True).raw))\n",
    "print(results)"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/metaclip-b32-400m\")\n",
    "model = AutoModel.from_pretrained(\"facebook/metaclip-b32-400m\")\n",
    "\n",
    "image = Image.open(\"docs/CLIP.png\")\n",
    "inputs = processor(text=[\"a diagram\", \"a dog\", \"a cat\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "  logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "  text_probs = logits_per_image.softmax(dim=-1)\n",
    "print(\"Label probs:\", text_probs)"
   ]
=======
>>>>>>> c6fa8c57de22142415714cfd27998149e99f6e50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
